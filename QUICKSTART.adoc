= QUICKSTART

=== Simple Transliteration Model

We implement the following simple transliteration model:

* a -> b, b -> c, c -> d, d -> e, ..., z -> a

The steps below are going to allow us to implement:

1. *design diagram*
2. *diagram based code*
  * generate code
  * run tests
2. *build transliteration data*
3. *neural network model*
  * train model
  * export model to onnx
4. *run ruby code*

=== Design Diagram

We have designed the following diagram with lucidchart:
 https://github.com/interscript/transliteration-learner-from-graphs/blob/main/learn-graph/resources/Model1.0.png[diagram]

It represents a simple strategy that we broke down into 3 steps:

* Transliteration, bundling the steps below
* Preprocessor: clean up chars, lower case, ...
* Mapping: applying the above transformation on characters

A strategy can be thought as a structure through which data "flows"
and gets processed by the various operations represented by the nodes (~functional programming).

In the design, the following conventions are currently supported:

1. *Entries*
    * represent: logic flows starts
    * "Curly Brace Note"
2. *Nodes*
    * represent: computational nodes specifying uniquely some operations
    * "Process", "Decision", "Terminator"
3. *Connections*
  * represent: logical steps
  * "unlabelled directed arrows" "labelled directed arrows"
   (in arbitrary number)

Entries are activated in the following ways:

1. specifying main entry when building code
2. calling entry via node: e.g. "Preprocessor"
3. or calling for a recursion or loop: e.g. "process each word with mapping"

So in the diagram, the computational flow is jumping between subdiagrams.

=== Transliteration Code Generation

Diagram designs on lucidchart can be exported as csv.


[source,sh]
----
# going to learn-graph repo.
cd learn-graph
# install deps
julia packageInstall.jl
julia train.jl --path-lucidchart-csv resources/FullDemo.csv --brain-entry transliteration --path-model resources/FullDemo.dat
----

The code should output a list of warning messages:

â”Œ Warning: ("unimplemented Node:: Id", 6, " Name: ", "map all letters utilising table and @ to a")

This means that the logic needs to be coded.
This is done under */learn-graph/src/Rules.jl*.


===== Code Snippets Implementation

Code building is based on the following ideas and tools:

[source,ruby]
----
# computation state
dataSTATE = Dict{String, Any}(
            "state" => nothing,
            "brain" => nothing);

# Dictionary with the commands
dicCODE = Dict{String, Functor}()
----

Next examples are basic node implementations:
[source,ruby]
----
# Node terminating computation
dicCODE["done, terminate"] =
    #===
        Basic form of functors:
            d: data
            e: dicBRAINS
            f: df_Nodes

        Inputs and Outputs are specified
        :in => "l_transliterated" # list of
        :out => "res" # field expected at end of (sub)sequence
    ===#
    Functor((d,e=nothing,f=nothing) ->
        begin
            d["res"] = d["txt"]
            d
        end, # identity
        Dict(:in => ["txt"],
             :out => ["res"]))

dicCODE["bind transliterated words together"] =
    #===
      Implimentation of simple node
    ===#
    Functor((d,e=nothing,f=nothing) ->
        begin
          # ["a", "cat"] -> "a cat"
          d["txt"] = join(d["l_transl_wrds"], " ");
          d
        end,
        Dict(:in => ["l_transl_wrds"],
             :out => ["txt"]))

----

Above, for more code stability, :in and :out fields necessary for the
computational flow to be performed most be specified.
"res" allow to terminate a (sub)flow returning a particular value rather than
the full computation state.

It can be useful to review how to call an other part of the
diagram and here also to loop over that process.

[source,ruby]
----
dicCODE["apply mappings on each word"] =
    Functor((d,e=nothing,f=nothing) ->
        begin
          d["l_transl_wrds"] =
            map(wrd ->
                begin
                    dd = copy(dataSTATE)
                    dd["wrd"] = wrd
                    interfaceName = "mapping"
                    node = e[interfaceName]
                    runAgent(node, e, f, dd)
                end,
                d["l_wrds"])
          d
        end,
        Dict(:in => ["l_wrds"],
             :out => ["l_transl_wrds"]))
----

More examples can be found under *learn-graph/resources/RulesSamples/*
and code and functions can be copied *learn-graph/src/Rules.jl*.

===== Create code from dir

Alternatively, code can be generated from multiple .csv files
as the ones in *learn-graph/resources/modelDir/*.
This approach allows for more  a more atomic approach, sub components
can be separated and tweaked.
[source,sh]
----
cd learn-graph
julia train.jl --dir-path-lucidchart-csv resources/modelDir/ --brain-entry preprocessor --path-model resources/DirDemo.dat
----

===== Run Python, external code and others

===== Run Tests and transliteration



=== Neural networks

===== Generate Transliteration Data
===== Train Neural Networks
===== Export Neural Networks to Onnx

=== Run ruby Code
